This is simple to use, simply relax your eyes to allow both images to drift together.

This tool starts from the pixel; the most basic point of light in an image. 

But instead of treating each pixel as a flat bit of color, It is treated as a living unit of perception. 

Every pixel contains two critical qualities: brightness and saturation. Brightness is used as a measure of presence or nearness, and saturation as a sense of density or form. Together, they become the raw material for a new kind of depth.

My tool reads these values across an image and creates a spatial field; not using traditional 3D geometry or camera-based depth maps.

The result is a depth matrix.

Here are the suggested values for creating depth in AndroidVR.

![image](https://github.com/user-attachments/assets/c45c08fe-7890-419c-9065-44ff2574c8e7)

**Notice**

Be VERY careful about using 3D flicker mode, this mode is intended for people who have mono vision and should not be used by those who have binocular vision due to the cognitive strain it puts on the user when input is received through both eyes.

**Transfer to Natural Vision**

**Temporary effects**: 10-30 minutes of aftereffects might occur immediately after 1-2 hour sessions

**Initial transfer**: Minor involuntary interpretation might begin after 15-20 hours of cumulative exposure (across multiple days)

**Measurable adaptation**: 30-50 hours of training over 3-6 weeks would be needed for your brain to begin consistently applying these cues in natural environments

**Robust integration**: 100+ hours over 2-4 months for substantial, persistent effects when not using the app
